import { useState, useRef, useCallback } from 'react'\nimport { toast } from 'react-hot-toast'\n\ninterface UseVoiceChatOptions {\n  websocket: WebSocket | null\n  onVoiceStart?: () => void\n  onVoiceEnd?: () => void\n  onResponseStart?: () => void\n  onResponseEnd?: () => void\n}\n\nexport function useVoiceChat({\n  websocket,\n  onVoiceStart,\n  onVoiceEnd,\n  onResponseStart,\n  onResponseEnd\n}: UseVoiceChatOptions) {\n  const [isListening, setIsListening] = useState(false)\n  const [isResponsePlaying, setIsResponsePlaying] = useState(false)\n  const [audioLevel, setAudioLevel] = useState(0)\n  \n  const mediaRecorder = useRef<MediaRecorder | null>(null)\n  const audioContext = useRef<AudioContext | null>(null)\n  const audioQueue = useRef<ArrayBuffer[]>([])\n  const isPlayingRef = useRef(false)\n  \n  // Convert audio buffer to base64 for sending to WebSocket\n  const arrayBufferToBase64 = (buffer: ArrayBuffer): string => {\n    const bytes = new Uint8Array(buffer)\n    let binary = ''\n    for (let i = 0; i < bytes.byteLength; i++) {\n      binary += String.fromCharCode(bytes[i])\n    }\n    return btoa(binary)\n  }\n  \n  // Convert base64 to audio buffer for playback\n  const base64ToArrayBuffer = (base64: string): ArrayBuffer => {\n    const binary = atob(base64)\n    const bytes = new Uint8Array(binary.length)\n    for (let i = 0; i < binary.length; i++) {\n      bytes[i] = binary.charCodeAt(i)\n    }\n    return bytes.buffer\n  }\n  \n  // Play audio response from OpenAI\n  const playAudioResponse = useCallback(async (audioData: string) => {\n    if (!audioContext.current) {\n      audioContext.current = new AudioContext()\n    }\n    \n    try {\n      const audioBuffer = base64ToArrayBuffer(audioData)\n      const decodedAudio = await audioContext.current.decodeAudioData(audioBuffer)\n      \n      const source = audioContext.current.createBufferSource()\n      source.buffer = decodedAudio\n      source.connect(audioContext.current.destination)\n      \n      source.onended = () => {\n        setIsResponsePlaying(false)\n        onResponseEnd?.()\n      }\n      \n      setIsResponsePlaying(true)\n      onResponseStart?.()\n      source.start()\n      \n      console.log('ðŸ”Š Playing AI response')\n    } catch (error) {\n      console.error('Error playing audio:', error)\n      toast.error('Failed to play audio response')\n      setIsResponsePlaying(false)\n    }\n  }, [onResponseStart, onResponseEnd])\n  \n  // Start recording audio\n  const startListening = useCallback(async () => {\n    if (!websocket || websocket.readyState !== WebSocket.OPEN) {\n      toast.error('Not connected to voice assistant')\n      return\n    }\n    \n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          sampleRate: 16000,\n          channelCount: 1,\n          echoCancellation: true,\n          noiseSuppression: true\n        }\n      })\n      \n      // Create audio context for processing\n      if (!audioContext.current) {\n        audioContext.current = new AudioContext({ sampleRate: 16000 })\n      }\n      \n      // Setup MediaRecorder for capturing audio\n      mediaRecorder.current = new MediaRecorder(stream, {\n        mimeType: 'audio/webm;codecs=opus',\n        audioBitsPerSecond: 16000\n      })\n      \n      const audioChunks: Blob[] = []\n      \n      mediaRecorder.current.ondataavailable = (event) => {\n        if (event.data.size > 0) {\n          audioChunks.push(event.data)\n        }\n      }\n      \n      mediaRecorder.current.onstop = async () => {\n        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' })\n        const arrayBuffer = await audioBlob.arrayBuffer()\n        \n        // Send audio to OpenAI via WebSocket\n        const base64Audio = arrayBufferToBase64(arrayBuffer)\n        \n        if (websocket && websocket.readyState === WebSocket.OPEN) {\n          websocket.send(JSON.stringify({\n            type: 'audio',\n            audio: base64Audio\n          }))\n          console.log('ðŸŽ¤ Sent audio to AI assistant')\n        }\n        \n        // Cleanup\n        stream.getTracks().forEach(track => track.stop())\n      }\n      \n      // Start recording\n      mediaRecorder.current.start(100) // Collect data every 100ms\n      setIsListening(true)\n      onVoiceStart?.()\n      \n      console.log('ðŸŽ¤ Started listening...')\n      toast('Listening...', { icon: 'ðŸŽ¤', duration: 1000 })\n      \n    } catch (error) {\n      console.error('Error starting voice recording:', error)\n      if (error instanceof Error && error.name === 'NotAllowedError') {\n        toast.error('Microphone permission denied. Please allow microphone access.')\n      } else {\n        toast.error('Failed to start voice recording')\n      }\n    }\n  }, [websocket, onVoiceStart])\n  \n  // Stop recording audio\n  const stopListening = useCallback(() => {\n    if (mediaRecorder.current && mediaRecorder.current.state !== 'inactive') {\n      mediaRecorder.current.stop()\n      setIsListening(false)\n      onVoiceEnd?.()\n      \n      console.log('ðŸ›‘ Stopped listening')\n      toast('Processing...', { icon: 'âš¡', duration: 1000 })\n    }\n  }, [onVoiceEnd])\n  \n  // Handle WebSocket messages for audio responses\n  const handleAudioResponse = useCallback((message: any) => {\n    if (message.type === 'audio_response' && message.audio) {\n      playAudioResponse(message.audio)\n    }\n  }, [playAudioResponse])\n  \n  return {\n    isListening,\n    isResponsePlaying,\n    audioLevel,\n    startListening,\n    stopListening,\n    handleAudioResponse,\n    playAudioResponse\n  }\n}\n